{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a92a2178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.23.7)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: streamlit in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.45.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from PyMuPDF) (1.23.7)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (1.7.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (1.4.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (4.25.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from streamlit) (6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.4.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.38.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2021.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install required packages\n",
    "!pip install PyMuPDF nltk sentence-transformers faiss-cpu numpy streamlit transformers torch tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ef9629f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: now Importing all necessary libraries\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "63f93941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the TextProcessor class\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF document\"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    \n",
    "    def extract_qa_from_txt(self, txt_path):\n",
    "        \"\"\"Extract Q&A pairs from text file\"\"\"\n",
    "        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Split by Q/A pattern\n",
    "        qa_pairs = re.split(r'Q\\.\\s*', content)[1:]\n",
    "        qa_list = []\n",
    "        \n",
    "        for pair in qa_pairs:\n",
    "            if 'A.' in pair:\n",
    "                question, answer = pair.split('A.', 1)\n",
    "                qa_list.append({\n",
    "                    'question': question.strip(),\n",
    "                    'answer': answer.strip()\n",
    "                })\n",
    "        return qa_list\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,;:?!-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Tokenize and lemmatize words in each sentence\n",
    "        processed_sentences = []\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent)\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "            processed_sentences.append(' '.join(lemmatized_words))\n",
    "            \n",
    "        return processed_sentences\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=200):\n",
    "        \"\"\"Split text into chunks of approximately chunk_size words\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            current_count += 1\n",
    "            \n",
    "            if current_count >= chunk_size:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_count = 0\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2016ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now Define the EmbeddingModel class\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"Generate embeddings for list of texts\"\"\"\n",
    "        return self.model.encode(texts, convert_to_tensor=False)\n",
    "    \n",
    "    def create_faiss_index(self, embeddings, index_path='faiss_index.idx'):\n",
    "        \"\"\"Create and save FAISS index\"\"\"\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        \n",
    "        # Convert to float32 numpy array if needed\n",
    "        if not isinstance(embeddings, np.ndarray):\n",
    "            embeddings = np.array(embeddings)\n",
    "        if embeddings.dtype != np.float32:\n",
    "            embeddings = embeddings.astype('float32')\n",
    "            \n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, index_path)\n",
    "        return index\n",
    "    \n",
    "    def load_faiss_index(self, index_path='faiss_index.idx'):\n",
    "        \"\"\"Load existing FAISS index\"\"\"\n",
    "        if os.path.exists(index_path):\n",
    "            return faiss.read_index(index_path)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "336bad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing T5 model...\n",
      "Attempting to load t5-small...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88094bac40b44f42be2593ea4998b8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4690d11ea004772804081fe6aa413bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514fd2c48e644197a4f06b14b91cdf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8e233ad2c34ae1b669e303de14b9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c783367c3714e4a86e93da1e1d1b78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb811e4162f47c0800434c46c0369a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully loaded t5-small\n",
      "AnswerGenerator ready with t5-small\n"
     ]
    }
   ],
   "source": [
    "# lets divide it into two part \n",
    "\n",
    "# ==================== Part 1: Model Loader ====================\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "print(\"Initializing T5 model...\")\n",
    "\n",
    "# Try loading progressively larger models\n",
    "MODEL_LOADED = None\n",
    "for model_name in ['t5-small', 't5-base']:\n",
    "    try:\n",
    "        print(f\"Attempting to load {model_name}...\")\n",
    "        test_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        test_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        print(f\"‚úì Successfully loaded {model_name}\")\n",
    "        MODEL_LOADED = model_name\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to load {model_name}: {str(e)[:100]}...\")\n",
    "\n",
    "if not MODEL_LOADED:\n",
    "    print(\" Could not load any T5 models. Answer refinement will be disabled.\")\n",
    "\n",
    "# ==================== Part 2: Answer Generator ====================    \n",
    "class AnswerGenerator:\n",
    "    def __init__(self, model_name=MODEL_LOADED):\n",
    "        self.available = bool(model_name)\n",
    "        if not self.available:\n",
    "            return\n",
    "            \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "            print(f\"AnswerGenerator ready with {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize AnswerGenerator: {e}\")\n",
    "            self.available = False\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        \"\"\"Generate refined answer or return original context if unavailable\"\"\"\n",
    "        if not self.available:\n",
    "            return context\n",
    "            \n",
    "        try:\n",
    "            input_text = f\"question: {question} context: {context}\"\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=150,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Answer generation failed: {e}\")\n",
    "            return context\n",
    "\n",
    "# Initialize (will self-configure based on what loaded)\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39bc4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL VERIFICATION ===\n",
      "Test Question: What is stress?\n",
      "Test Context: Stress is the body's natural response to challenges. It can be positive (eustress) or negative (distress).\n",
      "Generated Answer: Stress is the body's natural response to challenges. It can be positive (eustress) or negative (distress).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m context[:\u001b[38;5;241m500\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(context) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 3. Upgrade the answer generator\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m enhanced_answer_generator \u001b[38;5;241m=\u001b[39m \u001b[43mEnhancedAnswerGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_LOADED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# 4. Test with your actual Q&A pairs\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== ENHANCED GENERATION TEST ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'model_name'"
     ]
    }
   ],
   "source": [
    "# ==================== FINAL WORKFLOW INTEGRATION ====================\n",
    "\n",
    "# 1. First, verify the loaded model\n",
    "print(\"\\n=== MODEL VERIFICATION ===\")\n",
    "test_question = \"What is stress?\"\n",
    "test_context = \"Stress is the body's natural response to challenges. It can be positive (eustress) or negative (distress).\"\n",
    "print(f\"Test Question: {test_question}\")\n",
    "print(f\"Test Context: {test_context}\")\n",
    "\n",
    "if answer_generator.available:\n",
    "    generated = answer_generator.generate_answer(test_question, test_context)\n",
    "    print(f\"Generated Answer: {generated}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using simple mode - returning raw context\")\n",
    "    print(test_context)\n",
    "\n",
    "# 2. Improved Answer Generation with Quality Control\n",
    "class EnhancedAnswerGenerator(AnswerGenerator):\n",
    "    def postprocess_answer(self, raw_answer):\n",
    "        \"\"\"Clean and format the generated answer\"\"\"\n",
    "        # Remove repetitive phrases\n",
    "        if \"question:\" in raw_answer.lower():\n",
    "            raw_answer = raw_answer.split(\"question:\")[0]\n",
    "        \n",
    "        # Capitalize first letter\n",
    "        raw_answer = raw_answer.strip()\n",
    "        if len(raw_answer) > 1:\n",
    "            raw_answer = raw_answer[0].upper() + raw_answer[1:]\n",
    "        \n",
    "        # Ensure proper punctuation\n",
    "        if raw_answer and raw_answer[-1] not in {'.', '!', '?'}:\n",
    "            raw_answer += '.'\n",
    "            \n",
    "        return raw_answer\n",
    "\n",
    "    def generate_answer(self, question, context):\n",
    "        if not self.available:\n",
    "            return context\n",
    "            \n",
    "        try:\n",
    "            # Enhanced prompt engineering\n",
    "            input_text = (\n",
    "                f\"Generate a concise answer to this university-related question. \"\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Context: {context[:2000]}\\n\"\n",
    "                f\"Answer:\"\n",
    "            )\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                repetition_penalty=1.5\n",
    "            )\n",
    "            \n",
    "            raw_answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return self.postprocess_answer(raw_answer)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {str(e)[:200]}\")\n",
    "            return context[:500] + (\"...\" if len(context) > 500 else \"\")\n",
    "\n",
    "# 3. Upgrade the answer generator\n",
    "enhanced_answer_generator = EnhancedAnswerGenerator(model_name=MODEL_LOADED)\n",
    "\n",
    "# 4. Test with your actual Q&A pairs\n",
    "print(\"\\n=== ENHANCED GENERATION TEST ===\")\n",
    "test_cases = [\n",
    "    (\"Who teaches CSS101?\", \n",
    "     \"Faculty for CSS101: Ms. Garima Sharma, Dr. Sandeep Singh\"),\n",
    "     \n",
    "    (\"What is Dr. Singh's email?\", \n",
    "     \"Contact: sandeepsingh@ncuindia.edu\"),\n",
    "     \n",
    "    (\"What is stress?\", \n",
    "     \"Definition: Stress is the body's response to demands\")\n",
    "]\n",
    "\n",
    "for question, context in test_cases:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Answer: {enhanced_answer_generator.generate_answer(question, context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44eb2b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded t5-small for answer refinement\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - ULTRA-ROBUST VERSION (works without internet/model downloads)\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.available = False\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Try to load any available model\n",
    "        for model_name in ['t5-small', 't5-base']:\n",
    "            try:\n",
    "                self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "                self.available = True\n",
    "                print(f\"‚úì Loaded {model_name} for answer refinement\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not self.available:\n",
    "            print(\"‚ö†Ô∏è Running in simple mode (using direct text snippets)\")\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        \"\"\"Returns either refined answer or original context\"\"\"\n",
    "        if not self.available:\n",
    "            # Fallback: return the most relevant text chunk\n",
    "            return context\n",
    "            \n",
    "        try:\n",
    "            input_text = f\"question: {question} context: {context}\"\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=150,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except:\n",
    "            return context\n",
    "\n",
    "# Initialize (will automatically configure based on what's available)\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "440f13db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Can access Hugging Face\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"https://huggingface.co\", timeout=5)\n",
    "    print(\"‚úì Can access Hugging Face\" if response.status_code == 200 else \"‚úó Blocked from Hugging Face\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59307f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Using simple mode (direct text answers)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - 100% WORKING VERSION (online or offline)\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.available = False\n",
    "        self.device = torch.device('cpu')  # Force CPU to avoid CUDA issues\n",
    "        \n",
    "        # Try loading with multiple approaches\n",
    "        try:\n",
    "            # Attempt direct load (will work if previously downloaded)\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained('t5-small').to(self.device)\n",
    "            self.available = True\n",
    "            print(\"‚úì Answer refinement enabled\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Using simple mode (direct text answers)\")\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        if not self.available:\n",
    "            # Return the 3 most relevant sentences\n",
    "            sentences = context.split('. ')\n",
    "            return '. '.join(sentences[:3]) + ('.' if len(sentences) >=3 else '')\n",
    "        \n",
    "        try:\n",
    "            input_text = f\"question: {question} context: {context[:2000]}\"  # Limit context size\n",
    "            inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            outputs = self.model.generate(**inputs, max_length=150)\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except:\n",
    "            return context[:500]  # Fallback to first 500 chars\n",
    "\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d800221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.50.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: accelerate in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp39-cp39-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.7.0-cp39-cp39-win_amd64.whl (212.4 MB)\n",
      "   ---------------------------------------- 212.4/212.4 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 6.3/6.3 MB 6.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\~~rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.2 requires torch==2.1.2, but you have torch 2.7.0 which is incompatible.\n",
      "torchvision 0.16.2 requires torch==2.1.2, but you have torch 2.7.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate sentence-transformers\n",
    "!pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41e11fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Attempting to load Llama 2...\n",
      "‚ö†Ô∏è Could not load Llama 2: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https...\n",
      "üî∂ Falling back to simple mode (direct text answers).\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Llama 2 Version (with fallback)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.available = False\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Try loading Llama 2 (7B Chat) - smaller and better than T5\n",
    "        try:\n",
    "            print(\"üîÑ Attempting to load Llama 2...\")\n",
    "            model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            self.available = True\n",
    "            print(\"‚úÖ Llama 2 loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load Llama 2: {str(e)[:200]}...\")\n",
    "            print(\"üî∂ Falling back to simple mode (direct text answers).\")\n",
    "\n",
    "    def generate_answer(self, question, context):\n",
    "        if not self.available:\n",
    "            # Fallback: return the most relevant part of the context\n",
    "            sentences = context.split('. ')\n",
    "            return '. '.join(sentences[:3]) + ('.' if len(sentences) >= 3 else '')\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            [INST] <<SYS>>\n",
    "            You are a helpful university assistant. Answer the student's question using ONLY the provided context.\n",
    "            <</SYS>>\n",
    "            \n",
    "            Question: {question}\n",
    "            Context: {context[:3000]}  # Limit to avoid memory issues\n",
    "            [/INST]\n",
    "            \"\"\"\n",
    "            \n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output\n",
    "            return answer.split(\"[/INST]\")[-1].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Answer generation failed: {e}\")\n",
    "            return context[:500]  # Return first 500 chars as fallback\n",
    "\n",
    "# Initialize\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c155f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0811bb80bb544c4da806ea33bec8c5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Run this in a new cell\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb336d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using optimized offline mode (TF-IDF based)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - OFFLINE MODE (No Hugging Face models needed)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        print(\"‚úÖ Using optimized offline mode (TF-IDF based)\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        \n",
    "    def generate_answer(self, question, context):\n",
    "        \"\"\"Returns the most relevant part of the context using TF-IDF\"\"\"\n",
    "        try:\n",
    "            # Combine question and context for analysis\n",
    "            texts = [question] + context.split('. ')\n",
    "            \n",
    "            # Create TF-IDF matrix\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "            \n",
    "            # Calculate similarity between question and context sentences\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "            \n",
    "            # Get top 3 most relevant sentences\n",
    "            top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "            best_answers = [texts[i+1] for i in top_indices if similarities[i] > 0.1]\n",
    "            \n",
    "            return '. '.join(best_answers) if best_answers else context[:500]\n",
    "        except:\n",
    "            return context[:500]  # Fallback\n",
    "\n",
    "# Initialize\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96173cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Community Service course (CSS101) is taught by: Ms\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Who teaches Community Service?\"\n",
    "test_context = \"The Community Service course (CSS101) is taught by: Ms. Garima Sharma, Dr. Sandeep Singh, and Dr. Rita Chhikara. Classes are held on Mondays.\"\n",
    "\n",
    "print(answer_generator.generate_answer(test_question, test_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6510fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using enhanced offline mode (complete answers)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - IMPROVED OFFLINE VERSION (complete answers)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        print(\"‚úÖ Using enhanced offline mode (complete answers)\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        \n",
    "    def generate_answer(self, question, context):\n",
    "        \"\"\"Returns complete sentences with proper formatting\"\"\"\n",
    "        try:\n",
    "            # Split context into proper sentences (handling abbreviations)\n",
    "            sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', context) if s.strip()]\n",
    "            \n",
    "            # Add question to the text pool\n",
    "            texts = [question] + sentences\n",
    "            \n",
    "            # Create TF-IDF matrix\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "            \n",
    "            # Get top 3 relevant sentences (minimum similarity threshold)\n",
    "            top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "            best_answers = [sentences[i] for i in top_indices if similarities[i] > 0.1]\n",
    "            \n",
    "            # Formatting fixes\n",
    "            answer = ' '.join(best_answers)\n",
    "            answer = re.sub(r'\\s([,.?])', r'\\1', answer)  # Fix spaces before punctuation\n",
    "            answer = answer[0].upper() + answer[1:]        # Capitalize first letter\n",
    "            return answer if answer.endswith(('.','!','?')) else answer + '.'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug: {str(e)}\")  # Only visible during development\n",
    "            return context[:500]  # Fallback\n",
    "\n",
    "# Initialize\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d61a6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Community Service course (CSS101) is taught by: Ms.\n"
     ]
    }
   ],
   "source": [
    "test_context = \"The Community Service course (CSS101) is taught by: Ms. Garima Sharma, Dr. Sandeep Singh, and Dr. Rita Chhikara. Classes are held on Mondays.\"\n",
    "print(answer_generator.generate_answer(\"Who teaches CSS101?\", test_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c21000fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using ultra-reliable answer generation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - FINAL BULLETPROOF VERSION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        print(\"‚úÖ Using ultra-reliable answer generation\")\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        \n",
    "    def generate_answer(self, question, context):\n",
    "        \"\"\"Returns complete, well-formatted answers every time\"\"\"\n",
    "        try:\n",
    "            # Step 1: Better sentence splitting that handles titles (Ms., Dr., etc.)\n",
    "            sentences = [s.strip() for s in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)]\n",
    "            \n",
    "            # Step 2: Find the most relevant sentence\n",
    "            if len(sentences) == 0:\n",
    "                return context[:500]\n",
    "                \n",
    "            question_vec = self.vectorizer.fit_transform([question])\n",
    "            sentence_vecs = self.vectorizer.transform(sentences)\n",
    "            \n",
    "            similarities = cosine_similarity(question_vec, sentence_vecs).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_answer = sentences[best_idx]\n",
    "            \n",
    "            # Step 3: Ensure we return complete information\n",
    "            if \":\" in best_answer:  # If answer contains a list (like faculty names)\n",
    "                for s in sentences[best_idx+1:]:\n",
    "                    if not s[0].isupper():  # Continuation of the list\n",
    "                        best_answer += \" \" + s\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "            # Step 4: Final formatting\n",
    "            best_answer = best_answer.replace(\"  \", \" \")\n",
    "            if not best_answer.endswith(('.','!','?')):\n",
    "                best_answer += '.'\n",
    "                \n",
    "            return best_answer[0].upper() + best_answer[1:]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug: {str(e)}\")\n",
    "            return context[:500]\n",
    "\n",
    "answer_generator = AnswerGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fa1f9590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Community Service course (CSS101) is taught by: Ms. Garima Sharma, Dr. Sandeep Singh, and Dr. Rita Chhikara.\n"
     ]
    }
   ],
   "source": [
    "test_context = \"The Community Service course (CSS101) is taught by: Ms. Garima Sharma, Dr. Sandeep Singh, and Dr. Rita Chhikara. Classes are held on Mondays.\"\n",
    "print(answer_generator.generate_answer(\"Who teaches CSS101?\", test_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "006d3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, faiss_index, pdf_chunks, qa_embeddings):\n",
    "    # 1. First i am trying Q&A database\n",
    "    qa_answer = get_qa_answer(question, qa_embeddings)\n",
    "    if qa_answer:\n",
    "        return qa_answer\n",
    "    \n",
    "    # 2. Searching for the  PDF chunks if no Q&A match\n",
    "    similar_chunks = search_similar_text(question, faiss_index, pdf_chunks)\n",
    "    if similar_chunks:\n",
    "        context = ' '.join(similar_chunks[:3])  # Use top 3 chunks\n",
    "        return answer_generator.generate_answer(question, context)\n",
    "    \n",
    "    return \"I couldn't find information about this topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a56df945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSS101 classes are held in Block B, Room 205 every Monday and Wednesday.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"Where is the CSS101 class held?\"\n",
    "context = \"CSS101 classes are held in Block B, Room 205 every Monday and Wednesday.\"\n",
    "\n",
    "print(answer_generator.generate_answer(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7bb95fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Sandeep Singh (sandeepsingh@ncuindia.edu).\n",
      "Block B has faculty offices.\n"
     ]
    }
   ],
   "source": [
    "# Very short context\n",
    "print(answer_generator.generate_answer(\"Who is Dr. Singh?\", \"Dr. Sandeep Singh (sandeepsingh@ncuindia.edu)\"))\n",
    "\n",
    "# Long context\n",
    "long_text = \"The campus has 3 blocks. Block A has CS dept. Block B has faculty offices. Block C has labs.\"\n",
    "print(answer_generator.generate_answer(\"Where are faculty offices?\", long_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e0c2badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Bulletproof Answer Generator\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        sentences = [s.strip() for s in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)]\n",
    "        if not sentences:\n",
    "            return context[:500]\n",
    "            \n",
    "        question_vec = self.vectorizer.fit_transform([question])\n",
    "        sentence_vecs = self.vectorizer.transform(sentences)\n",
    "        \n",
    "        similarities = cosine_similarity(question_vec, sentence_vecs).flatten()\n",
    "        best_idx = np.argmax(similarities)\n",
    "        best_answer = sentences[best_idx]\n",
    "        \n",
    "        # Handle list continuations (e.g., faculty names)\n",
    "        if \":\" in best_answer:\n",
    "            for s in sentences[best_idx+1:]:\n",
    "                if not s[0].isupper():  # Continuation line\n",
    "                    best_answer += \" \" + s\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "        return best_answer[0].upper() + best_answer[1:] + ('' if best_answer.endswith(('.','!','?')) else '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a5328ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Complete Chatbot Engine\n",
    "class CollegeChatbot:\n",
    "    def __init__(self, pdf_path, qa_path):\n",
    "        self.processor = TextProcessor()\n",
    "        self.answer_gen = AnswerGenerator()\n",
    "        \n",
    "        # Load data\n",
    "        self.pdf_text = self.processor.extract_text_from_pdf(pdf_path)\n",
    "        self.qa_pairs = self.processor.extract_qa_from_txt(qa_path)\n",
    "        \n",
    "        # Prepare FAISS index\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.pdf_chunks = self.processor.chunk_text(self.pdf_text)\n",
    "        self.qa_questions = [qa['question'] for qa in self.qa_pairs]\n",
    "        \n",
    "        # Combine all texts for embedding\n",
    "        all_texts = self.pdf_chunks + self.qa_questions\n",
    "        self.embeddings = self.model.encode(all_texts)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = faiss.IndexFlatL2(self.embeddings.shape[1])\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "    def query(self, question):\n",
    "        # 1. Check Q&A pairs first\n",
    "        question_embed = self.model.encode([question])\n",
    "        qa_similarities = np.dot(self.embeddings[len(self.pdf_chunks):], question_embed.T).flatten()\n",
    "        best_qa_idx = np.argmax(qa_similarities)\n",
    "        \n",
    "        if qa_similarities[best_qa_idx] > 0.7:  # Similarity threshold\n",
    "            return self.qa_pairs[best_qa_idx]['answer']\n",
    "        \n",
    "        # 2. Search PDF chunks\n",
    "        _, pdf_indices = self.index.search(question_embed.astype('float32'), k=3)\n",
    "        context = ' '.join([self.pdf_chunks[idx] for idx in pdf_indices[0] if idx < len(self.pdf_chunks)])\n",
    "        \n",
    "        return self.answer_gen.generate_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "074d543f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'Aptitude Exam Preparation Guide(1).pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [112]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m PDF_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAptitude Exam Preparation Guide(1).pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m      4\u001b[0m QA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestionnanswers.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m bot \u001b[38;5;241m=\u001b[39m \u001b[43mCollegeChatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDF_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Test it\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(bot\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho teaches Community Service?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36mCollegeChatbot.__init__\u001b[1;34m(self, pdf_path, qa_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_gen \u001b[38;5;241m=\u001b[39m AnswerGenerator()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mextract_qa_from_txt(qa_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare embeddings\u001b[39;00m\n",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36mTextProcessor.extract_text_from_pdf\u001b[1;34m(self, pdf_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_text_from_pdf\u001b[39m(\u001b[38;5;28mself\u001b[39m, pdf_path):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract text from PDF document\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mfitz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\fitz\\fitz.py:4118\u001b[0m, in \u001b[0;36mDocument.__init__\u001b[1;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[0;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m   4117\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 4118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m   4119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename):\n\u001b[0;32m   4120\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is no file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: no such file: 'Aptitude Exam Preparation Guide(1).pdf'"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialize and Run\n",
    "# Set your file paths here\n",
    "PDF_PATH = \"Aptitude Exam Preparation Guide(1).pdf\"  \n",
    "QA_PATH = \"questionnanswers.txt\"\n",
    "\n",
    "bot = CollegeChatbot(PDF_PATH, QA_PATH)\n",
    "\n",
    "# Test it\n",
    "print(bot.query(\"Who teaches Community Service?\"))\n",
    "print(bot.query(\"What is the schedule for CSS101?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2c96dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        try:\n",
    "            # Handle empty context\n",
    "            if not context.strip():\n",
    "                return \"I couldn't find information about this topic.\"\n",
    "                \n",
    "            # Improved sentence splitting\n",
    "            sentences = [s.strip() for s in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context) if s.strip()]\n",
    "            \n",
    "            if not sentences:\n",
    "                return context[:500] + ('...' if len(context) > 500 else '')\n",
    "            \n",
    "            # Get most relevant sentence\n",
    "            question_vec = self.vectorizer.fit_transform([question])\n",
    "            sentence_vecs = self.vectorizer.transform(sentences)\n",
    "            similarities = cosine_similarity(question_vec, sentence_vecs).flatten()\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_answer = sentences[best_idx]\n",
    "            \n",
    "            # Handle list continuations\n",
    "            if \":\" in best_answer:\n",
    "                for s in sentences[best_idx+1:]:\n",
    "                    if not s[0].isupper() and len(s.split()) < 10:  # Continuation line\n",
    "                        best_answer += \" \" + s\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # Final formatting (bulletproof)\n",
    "            if not best_answer:\n",
    "                return context[:500] + ('...' if len(context) > 500 else '')\n",
    "                \n",
    "            best_answer = best_answer.replace(\"  \", \" \").strip()\n",
    "            if not best_answer:\n",
    "                return context[:500] + ('...' if len(context) > 500 else '')\n",
    "                \n",
    "            # Capitalize and punctuate\n",
    "            if not best_answer[0].isupper():\n",
    "                best_answer = best_answer[0].upper() + best_answer[1:]\n",
    "            if not best_answer.endswith(('.','!','?')):\n",
    "                best_answer += '.'\n",
    "                \n",
    "            return best_answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug Error: {str(e)}\")\n",
    "            return context[:500] + ('...' if len(context) > 500 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2b794eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Community Service (CSS101) course is taught by Ms. Garima Sharma, Dr. Sandeep Singh, Dr. Rita Chhikara, Dr. Anvesha Katti, and Dr. Yogita Gigra.\n",
      "I couldn't find relevant information in the documents.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialize and Run\n",
    "# Setting my file paths here\n",
    "PDF_PATH = \"Aptitude Exam Preparation Guide.pdf\"  \n",
    "QA_PATH = \"questionnanswers.txt\"\n",
    "\n",
    "bot = CollegeChatbot(PDF_PATH, QA_PATH)\n",
    "\n",
    "# Test it\n",
    "print(bot.query(\"Who teaches Community Service?\"))\n",
    "print(bot.query(\"What is the schedule for CSS101?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d898d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\HP\\Downloads\n",
      "Files present: ['!DOCTYPE html.html', '!DOCTYPE.html', '(21csu278)Assignment 1 DE&CA.pdf', '(Aditya Sindhu )[Problem Statement 7](21csu278).docx', '(Aditya Sindhu)Document.pdf', '.env.ipynb', '.ipynb_checkpoints', '.opera', '.~banklist.csv.xlsx', '01_Drill_Functions.ipynb', '01_Drill_Numpy-2 (1).ipynb', '01_Drill_Numpy-2.ipynb', '01_Drill_Variables (1) (1).ipynb', '01_Drill_Variables (1).ipynb', '01_Drill_Variables (2) (1).ipynb', '01_Drill_Variables (2).ipynb', '01_Drill_Variables (3).ipynb', '01_Drill_Variables.ipynb', '01_Fashion MNIST Problem.ipynb', '01_Missing Data now.ipynb', '01_Notebook_control_struct.ipynb', '01_Notebook_Data and Expressions.ipynb', '01_Notebook_Distribution Plots.ipynb', '01_Notebook_Matplotlib Concepts.ipynb', '01_Notebook_Numpy Arrays (1).ipynb', '01_Notebook_Numpy Arrays (2).ipynb', '01_Notebook_Numpy Arrays.ipynb', '01_Project_Dataset Link (2) (1).txt', '01_Project_Dataset Link (2) (2).txt', '01_Project_Dataset Link (2) (3).txt', '01_Project_Dataset Link (2) (4).txt', '01_Project_Dataset Link (2) (5).txt', '01_Project_Dataset Link (2).txt', '01_Project_Failed Banks (1).ipynb', '01_Project_Failed Banks.ipynb', '01_python code.ipynb', '01_python project Web scraping.ipynb', '01_Self_test_dict (1).ipynb', '01_Self_test_dict.ipynb', '02. The French Revolution Author Thomas Carlyle.pdf', '02. World War One at Home author BBC.pdf', '02_Celsius to Fahrenheit.csv', '02_Celsius to Fahrenheit.xlsx', '02_COLAB_TensorFlow_2_0_Introduction (4).ipynb', '02_Drill_Decisions-1 (1).ipynb', '02_Drill_Decisions-1.ipynb', '02_Drill_Function (1).ipynb', '1 Basic_Image_Processing.ipynb', '1. Learn About the United States Author United States Citizenship and Immigration Services.pdf', '1.jpg', '10000000_662098952474184_2584067087619170692_n.pdf', '12345 (2).png', '12_22.jpg', '12_38.jpg', '1605.07683v4.pdf', '1924-Article Text-1228-1-10-20201228.pdf', '1e29e5966a43f93a1f814f181f7d7198.jpg', '1_harry_potter_qa.json', '20-01(exp1).ipynb', '2023 AIML PROJECT.ipynb', '20241.jpg', '20242.jpg', '20243.jpg', '20244.png', '20245.jpg', '202456.png', '20246.jpg', '21CSU092.xlsx', '21CSU254_Aditya(DS lab manual).docx', '21CSU270_Aviral Jain_FOCP II_Lab_manual-2.docx', '21csu278 21csu283  21csu326 Into.AIML project (1).ipynb', '21csu278 21csu283  21csu326 Into.AIML project (2).ipynb', '21csu278 21csu283  21csu326 Into.AIML project.ipynb', '21csu278 cs form .pdf', '21csu278 GP form.pdf', '21csu278_Aditya Sindhu Cs form-7th sem 2024.pdf', '21csu278_Nlp final lab manual.pdf', '21CSU326_EXP_8_FASHION MNIST-CNN.ipynb', '21CSU326_EXP_9_cipar-10 CNN.ipynb', '22Nov24.ipynb', '3010.House.of.the.Dragon.S01E06.1080p.WEBRip.Dual.Latino.mkv', 'AAIES10.ipynb', 'AAIES11 (1).ipynb', 'AAIES11.ipynb', 'AAIES9.ipynb', 'ACS CLASS WORK', 'aditya sindhu 08_24 resume (1).pdf', 'aditya sindhu 08_24 resume.pdf', 'Aditya Sindhu Resume.pdf', 'ADITYA SINDHU.pdf', 'Aditya Sindhu01_Resume.pdf', 'Aditya Sindhu_21csu278 CS-Form 7th sem.pdf', 'Aditya Sindhu_21csu278(case studyACS).ipynb', 'Aditya Sindhu_CSE(AI).pdf', 'Aditya Sindhu_Resume.pdf', 'aditya.pdf', 'AdityaSindhu 21csu278.pptx', 'ADITYA_Types of Ecosystem.pdf', 'agenda-easy.jsonl', 'ai image 1.jpeg', 'ai image 11.jpg', 'ai image 111.jpg', 'ai image 2.jpeg', 'ai image 22.jpg', 'ai image 3', 'ai image 33.jpg', 'ai image 4.jpg', 'ai image 6.jpg', 'ai image 7.jpg', 'AI-Based_Personalized_E-Learning_Systems_Issues_Challenges_and_Solutions.pdf', 'ai..gz', 'ai.html', 'AIMLPROJECTWORK.ipynb', 'airbnb-easy.jsonl', 'airbnb-hard.jsonl', 'alice.txt', 'all - Copy.jsonl', 'all.jsonl', 'AnexploratorycasestudyRobertIlesFINALPDF.pdf', 'annotated-NLP_LabManual5D20Copy.pdf', 'apache-tomcat-10.1.34-windows-x64.zip', 'apache-tomcat-9.0.98.exe', 'Aptitude Exam Preparation Guide (1).pdf', 'Aptitude Exam Preparation Guide.pdf', 'Assignment 1.pdf', 'Assignment 5.pdf', 'Assignment 6 AAIES.ipynb', 'Assignment 6.pdf', 'Assignment 7.pdf', 'Assignment 8.pdf', 'audio.zip', 'babyrobot (2) - Copy.ipynb', 'babyrobot (2).ipynb', 'babyrobot (3) - Copy.ipynb', 'banklist.csv.ipynb', 'banklist.csv.xlsx', 'banklist_files', 'Basic concepts.ipynb', 'beige-leafy-watercolor-background.zip', 'bhabha_the other question.pdf', 'Big data', 'Book 6.pdf', 'Book 6.xlsx', 'bootstrap-4.6.2-dist.zip', 'card_transaction.v1.csv', 'certificate for internship.jpg', 'chatbot for first year students.html', 'chatbot for first year students.txt', 'chatbotquestions.txt.html', 'CodeTantra.SEA-x64-3.0.3.exe', 'coffee-easy.jsonl', 'coffee-hard.jsonl', 'com-mod-axesinmotion-racing-mod-apk-7-0-1-73526.apk', 'com.dvloper.granny.Napkforpc.com.xapk', 'comments.docx', 'compositional semantics.pdf', 'Computer_vision_epipolar_geo_Zisserman.pdf', 'Conversation of dl.txt', 'convo robox.txt', 'copy of image 11.jpg', 'Copy of Rowling, J.K - Harry Potter 04 - The Goblet of Fire (Rowling, J.K [Rowling, J.K]) (Z-Library) - Copy.pdf', 'Copy_of_Generating_Report_Cards_from_Excel_(PDF_form).ipynb', 'copy_of_generating_report_cards_from_excel_(pdf_form).py', 'creditcard.csv.zip', 'creditcard2 (1).xlsx', 'creditcard2 (2).xlsx', 'creditcard2.xlsx', 'cs', 'Cs-Hours form 2024 7-sem Aditya Sindhu 21csu278.pdf', 'CSE Secotion E result', 'CYBER SECURITY', 'd1.png', \"DALL¬∑E 2024-10-23 12.50.01 - A detailed flowchart illustrating the workflow for fine-tuning a GPT model and querying it in Google Colab. The flowchart begins with 'Upload Dataset'.webp\", 'data', 'Data Set for ML.zip', 'Data Set for ML11.zip', 'DataScienceAISampleQuestionPaper.pdf', 'Dataset for exp(10).zip', 'data_stackmathqa100k_answer_count_frequencies - Copy.txt', 'data_stackmathqa100k_answer_count_frequencies.txt', 'data_stackmathqa100k_source_counts (1).txt', 'data_stackmathqa100k_source_counts - Copy.txt', 'data_stackmathqa100k_source_counts.txt', 'da_2024.pdf', 'dblp-easy.jsonl', 'dblp-hard.jsonl', 'Deep learning', 'Deep Learning Lab Manual.docx', 'Deep Learning Practical-1.pdf', 'Deep Learning with Python.pdf', 'desktop.ini', 'Detailed Workflow.docx', 'Detailed Workflow.pdf', 'dev (1).csv', 'dev.csv', 'dfg.html', 'DGS.pdf', 'diabetes.csv', 'diabetes2.0.csv', 'dialogs.txt.txt', 'DL Lab manual 21csu278 (1).docx', 'DL Lab manual 21csu278 (2).docx', 'DL Lab manual 21csu278.docx', 'DL Manual _21csu385.docx', 'DL Manual _21csu385.pdf', 'dl1.png', 'dl10.png', 'dl11.png', 'dl2.png', 'dl3 (1) - Copy.ipynb', 'dl3 (1).ipynb', 'dl3.png', 'dl4.png', 'dl5.png', 'dl6.png', 'dl7.png', 'dl8.png', 'dl9.png', 'DL_lab_manual_21csu288.docx', 'dl_ppt.ipynb', 'DOC-20240927-WA0000..docx', 'DocScanner 24 Dec 2024 3-28 pm.pdf', 'DocScanner 24 Dec 2024 3-42 pm(1).pdf', 'DocScanner 24 Dec 2024 3-42 pm.pdf', 'DocScanner 24 Dec 2024 3-49 pm(1).pdf', 'DocScanner 24 Dec 2024 3-49 pm.pdf', 'Document (4).docx', 'Document 16.pdf', 'Document 17.pdf', 'Document 21.docx', 'Document 23.pdf', 'Document 3.pdf', 'Document 4.pdf', 'Draw it', 'drive-download-20240924T154455Z-001.zip', 'easy ai dataset', 'eclipse-inst-jre-win64 (1).exe', 'eclipse-inst-jre-win64.exe', 'emotions.zip', 'Empathy Ai.html', 'EXP-3 (1).ipynb', 'EXP-3.ipynb', 'EXP-7.ipynb', 'Exp5os.ipynb', 'Experiment No 1 cn.docx', 'Experiment-10  .ipynb', 'Experiment-11  .ipynb', 'Experiment-4 21csu278.docx', 'Experiment-5,21csu278.docx', 'Experiment-8  .ipynb', 'Experiment-9  .ipynb', 'Experiment3,21csu278.docx', 'Exploring_the_Feasibility_and_Efficacy_of_ChatGPT3.pdf', 'EXP_6.ipynb', 'EXP_7.ipynb', 'EXP_8.ipynb', 'EXP_9.ipynb', 'f61b05ea5c05e9a86cab699ed189f942.jpg', 'Feature_engg_pre_processing_python.pptx', 'fess1dd.zip', 'ff.txt', 'ffmpeg-7.0.1.tar.xz', 'Final Effective_Communication_I-Intro_Day_One 2021.ppt', 'Final project plaining.docx', 'Final Project ppt.pptx', 'Final project report synopsis (1).docx', 'Final project report synopsis (2).docx', 'Final project report synopsis.docx', 'Final project report synopsis.pdf', 'final year dataset', 'Final year research paper summery  (1).docx', 'Final year research paper summery .docx', 'final_all_select1000.json', 'fine tune data.json', 'fine tune data.txt', 'flight-easy.jsonl', 'flight-hard.jsonl', 'Flip flops ASU.zip', 'FOCP(LAB5 ).docx', 'freepik__image-of-a-boy-age-about-18-and-its-pet-cow-who-ar__30181.jpeg', 'freepik__modern-style-detailled-illustration-21-year-old-be__37175.jpeg', 'Game_of_Thrones_S01_EP10_-_Fire_and_Blood_Dual_Audio_Hindi_(FilmyZilla.vin).mp4', 'GATE _DA_2025_Syllabus.pdf', 'GATE_Admit Card.pdf', 'genda-hard.jsonl', 'General Proficiency form (1).docx', 'General Proficiency form.docx', 'GENERAL PROFICIENCY-STUDENTS EVALUATION_2019 (1)-1 (1).docx', 'GENERAL PROFICIENCY-STUDENTS EVALUATION_2019 (1)-1 (2).docx', 'GENERAL PROFICIENCY-STUDENTS EVALUATION_2019 (1)-1.docx', 'gess1dd.zip', 'go.zip', 'Granny_1.8.1_Apkpure.apk', 'Graph (1).java', 'Graph.java', 'Graph.java.java', 'greedy 5.pptx', 'Group 2_Schedule.pdf', 'Group Vs Ungroup Data -1.pptx', 'Groupwise_Roll_List_CSE.pdf', 'Growth strategies ppt.pptx', 'gsm8k-easy.jsonl', 'Hackathonslist.xlsx', 'happy-independence-day-aditya-sindhu-21csu278.png', 'happy-independence-day.png', 'HappyMod-3-1-1.apk', 'hard ai dataset', 'Harry.Potter.and.the.Sorcerers.Stone.(2001).720p.Hindi.Eng.MoviesFlix.NeT.mkv', 'harry_potter_qa.json', 'hess2dd.zip', 'HistoricalQuotes.csv', 'history dataset.zip', 'Hitesh Kumar 21CSU389 DBMS PROBLEM 7.pdf', 'hitesh yadav 2nd year cs form.docx', 'Hitesh yadav gp form 4th sem.docx', 'Hotel manag sql.sql', 'hotel2 sql.sql', 'Housing.csv', 'HRM - Meaning & Functions.pptx', 'html-css-course-master.zip', 'https%3a%2f%2fmirrors.163.com%2fcygwin%2f', 'HVPE Project.pdf', 'HVPE Project.pptx', 'IAIML PROJECT REPORT.docx', 'ideaIC-2021.3.3.exe', 'ideaIC-2023.3.2.exe', 'IJCTT-V71I4P114.pdf', 'IL_EX19_CS1-4a_FirstLastName_1.xlsx', 'image.png', 'IMG-20230502-WA0017.jpg', 'IMG-20230505-WA0000.jpg', 'IMG-20230505-WA0001.jpg', 'IMG20230312092330.jpg', 'IMG20230312093505.jpg', 'IMG_20230312_092913.jpg', 'IMG_20230428_112416.jpg', 'IMG_20230428_112444.jpg', 'IMG_20230502_225934.jpg', 'IMG_20230502_233553.jpg', 'IMG_20230503_000757.jpg', 'IMG_20230504_101400 (1).jpg', 'IMG_20230504_101400.jpg', 'IMG_20230505_074603.jpg', 'IMG_20230505_074625.jpg', 'IMG_20230505_074647.jpg', 'IMG_20230505_074714.jpg', 'IMG_20230505_074802.jpg', 'IMG_20230929_145358-1 (1).jpg', 'IMG_20230929_145358-1.jpg', 'IMG_20230929_145358.jpg', 'IMG_20231129_101201.jpg', 'import image.html', 'INDEX.docx', 'index.html', 'Inspiring Biopic Movie Poster (1).png', 'Inspiring Biopic Movie Poster (2).png', 'Inspiring Biopic Movie Poster.png', 'Instagram Intro Video Template - Made with Clipchamp (1).mp4', 'Instagram Intro Video Template - Made with Clipchamp.mp4', 'Intent.txt.json', 'Intor.AIML', 'Introduction To Python (1).ipynb', 'Introduction To Python.ipynb', 'Introduction-to-Human-and-Object-Detection-Robot.pptx', 'Introduction-to-RNN-Models (1).pptx', 'Introduction-to-RNN-Models.pptx', 'Introduction.ipynb', 'Iris.csv', 'java.zip', 'javascript.zip', 'jdk-17_windows-x64_bin.exe', 'jdk-18_windows-x64_bin.zip', 'jdk-20_windows-x64_bin.exe', 'jdk-21_windows-x64_bin.msi', 'jdk-22_windows-x64_bin.msi', 'jess3dd.zip', 'kali-linux-2022.4-installer-amd64.iso', 'kali-linux-2022.4-vmware-amd64.7z', 'kali-linux-2023.1-installer-amd64.iso', 'kali-linux-2023.1-virtualbox-amd64.7z', 'keerti kholiu.ipynb', 'KEERTI_CY_PPT.pptx', 'L 16 Unit-5 Air Pollution.pdf', 'L1-Basics of DC circuits.pdf', 'L1_introduction.pptx', 'L21-22Unit-5 Case Study-Air Pollution, Bhopal Gas Tragedy, Minamata Disease, Itai-Itai Disease, Blue Baby Syndrome (1).pdf', 'L2_howAIevolved.pptx', 'L3_AI.pptx', 'L4 - Supervised and Unsupervised Learning (1).pptx', 'L5 Reinforcement Learning and Types of data.pptx', 'Lab 1 21csu278.docx', 'Lab 3 21csu278.docx', 'Lab 4 21csu278.docx', 'Lab manual.docx', 'Lab manual_ACS1-1.docx', 'Lab report of Assignment 7(AAIES)  Aditya Sindhu.docx', 'lab work 212.docx', 'Lab_Manual - AI&ML_CSL236-3.docx', 'Language Lab Manual-semester I (1).docx', 'LearningPower_MindMap.gif', 'Lect10_Google Dorks.pdf', 'Lecture 1 - Database Systems.pdf', 'Lecture 2 - RDBMS.pdf', 'Lecture 3 - Relational Model.pdf', 'Lecture 4 - Integrity Constraints.pdf', 'Lecture 5 - Database Design.pdf', 'Lecture 6 - Semantics of Relationships.pdf', 'Lecture2_Role_of_algos 4th sem.pptx', 'Lecture3 role of algo 4th sem.pptx', 'Lecture_15_Markov Reward Process-1.pptx', 'Lecture_6_Introduction_to_RL.pptx', 'Lec_ARP-RARP.pptx', 'Lec_congestion_control (1).pptx', 'Lec_congestion_control.pptx', 'Lec_DHCP.pptx', 'Lec_DNS.pptx', 'Lec_FTP.pptx', 'Lec_ICMP.pptx', 'Lec_MAC.pptx', 'Lec_TCP-UDP.pptx', 'lehs1dd.zip', 'lehs2dd.zip', 'lehs3dd.zip', 'lib', 'libraries_data_management.ipynb', 'LLM2024.pdf', 'LLM_Models (1).ipynb', 'LLM_Models (2).ipynb', 'LLM_Models (3).ipynb', 'LLM_Models (4).ipynb', 'LLM_Models.ipynb', 'Lokesh21csu405(case studyACS).ipynb', 'L_Notebook_DataFrames (1).ipynb', 'L_Notebook_DataFrames.ipynb', 'MATH.tar', 'MathQA.zip', 'Maths practical 1 (278).docx', 'Maths practical 1 (465).docx', 'Maths Practicals.pdf', 'maths2.zip', 'MATLAB 21csu284.docx', 'Medieval France, a companion to French studies Author Arthur Tilley M.A.pdf', 'Mentor.ipynb', 'mentor_dataset.json', 'mentor_faiss_index.idx', 'merged_video (1).mp4', 'merged_video (2).mp4', 'merged_video.mp4', 'merged_video_with_extension.mp4', 'meta.json', 'MicrosoftTeams-image (1).png', 'MicrosoftTeams-image (10).png', 'MicrosoftTeams-image (2).png', 'MicrosoftTeams-image (3).png', 'MicrosoftTeams-image (4).png', 'MicrosoftTeams-image (5).png', 'MicrosoftTeams-image (6).png', 'MicrosoftTeams-image (7).png', 'MicrosoftTeams-image (8).png', 'MicrosoftTeams-image (9).png', 'MicrosoftTeams-image.png', 'MIND MAP.docx', 'mm.jpg.png', 'mnist.npz', 'mongodb-windows-x86_64-6.0.5-signed (1).msi', 'mongodb-windows-x86_64-6.0.5-signed.msi', 'mongosh-1.8.0-win32-x64.zip', 'MS Excel training- Practice file- Ira Edu-Tech.xlsx', 'MSB (1).exe', 'MSB.exe', 'MySQ-64-Bit-5.5.13.zip', 'National_Stock_Exchange_of_India.csv', 'NBandit (1).ipynb', 'NBandit.ipynb', 'NCU Orientation and Induction Program.ics', 'NCU-FRM-81-COMMUNITY-SERVICE-TRACKING-FORM(3) (1).docx', 'NCU-FRM-81-COMMUNITY-SERVICE-TRACKING-FORM(3) (1).pdf', 'NCU-FRM-81-COMMUNITY-SERVICE-TRACKING-FORM(3) (2).pdf', 'NCU-FRM-81-COMMUNITY-SERVICE-TRACKING-FORM(3).docx', 'NCU-FRM-81-COMMUNITY-SERVICE-TRACKING-FORM(3).pdf', 'NCU_CSL347_L13_Shivam (1).pptx', 'NCU_CSL347_L13_Shivam.pptx', 'Nessus-10.5.0-x64.msi', 'netflix_titles.csv.zip', 'New Microsoft PowerPoint Presentation (1).pptx', 'New Microsoft PowerPoint Presentation.pptx', 'New Section 1 (1).one', 'New Section 1.one', 'NIPS-2017-attention-is-all-you-need-Paper (1).pdf', 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'Nlp assignment 2(21csu278).pdf', 'nlp project report 2024.pdf', 'nlp project report.pdf', 'NLP_Assignment-2 (2).pdf', 'NLP_LabManual[1] - Copy (1).docx', 'NLP_LabManual[1] - Copy (2).docx', 'NLP_LabManual[1] - Copy.docx', 'NLP_LabManual[1] - Copy.pdf', 'NLP_LabManual[1].docx', 'NLP_Project.docx', 'nlp_project_888.ipynb', 'NLP_PROJECT_REPORT_21CSU_278_311_326 (1).pdf', 'NLP_PROJECT_REPORT_21CSU_278_311_326 (2).pdf', 'NLP_PROJECT_REPORT_21CSU_278_311_326 (3).pdf', 'NLP_PROJECT_REPORT_21CSU_278_311_326(no plag) (1) (1).docx', 'NLP_PROJECT_REPORT_21CSU_278_311_326(no plag) (1) (2).docx', 'NLP_PROJECT_REPORT_21CSU_278_311_326(no plag) (1).docx', 'NLP_PROJECT_REPORT_21CSU_278_311_326(no plag) (1).pdf', 'NLP_PROJECT_REPORT_21CSU_278_311_326(no plag).docx', 'NLP_PROJECT_REPORT_21CSU_278_311_326.docx', 'NLP_PROJECT_REPORT_21CSU_278_311_326.pdf', 'nmap-7.93-setup.exe', 'noc23-hs16.pdf', 'NOC24CS124S1051100773.pdf', 'nocoderegression mode234.png', 'nocoderegression mode345.png', 'nocoderegression model.png', 'Open Elective courses_July-Dec_2023_for_CSE.xlsx', 'openai_key.env.txt', 'OperaGXSetup.exe', 'OperaSetup (1).exe', 'OperaSetup.exe', 'Operations Management unit 1.pptx', 'OS', 'OS EXP-6.ipynb', 'OS Lab manual Aditya Sindhu(21csu278).docx', 'OS Workbook CSL303.docx', 'painting-mountain-lake-with-mountain-background.jpg', 'password.txt', 'Pass_1234_Setup (1).rar', 'Pass_1234_Setup.rar', 'pencard.jpg', 'Personalized_and_Adaptive_Learning.pdf', 'php.zip', 'picture12.jpg.png', 'Pictures', 'Planing for the final project.docx', 'Policyiteration (3).ipynb', 'Policyiteration.ipynb', 'Prac9.ipynb', 'practial 2 maths (278).docx', 'practial 2 maths (465).docx', 'PRACTICAL 3 MATHS 463.docx', 'Practical 3-converted (1).pdf', 'Practical 3-converted.pdf', 'Practical 3.docx', 'practical 4.docx', 'prdt development.pptx', 'Presentation 4 (1).pptx', 'Presentation 4.pptx', 'Print.pdf', 'Probability_1.ipynb', 'Product and Brand Management (Unit 1) (1).pptx', 'Product and Brand Management U2.pptx', 'Product and Brand Management U4.pptx', 'Projects-20240722T093004Z-001.zip', 'PyScripter-4.1.1-x64-Setup.exe', 'Python Project 3rd Semester.ipynb', 'PYTHON PROJECT REPORT 21csu278,21csu260,21csu254,21csu241.pdf', 'python-3.10.5-amd64.exe', 'python-3.11.4-amd64.exe', 'python.zip', 'python_resources.ipynb', 'qaz.pdf', 'qa_hp.json', 'Qlearningmontecarlo (1).ipynb', 'qrcode_in.mathworks.com.png', 'qrcode_padlet.com.png', 'qrcode_www.mycamu.co.in.png', 'qrcode_www.netflix.com.png', 'questionnanswers.txt', 'questions and answers of book 1.json', 'qwer', 'qwer.jpg', 'qwer1.jpg', 'qwer12.jpg', 'qwer123.jpg', 'qwer2.jpg', 'qwer3', 'qwer4', 'qwerasd.html', 'R453Y48ApplicationForm.pdf', 'R453Y48PhotoId.pdf', 'R487E44ApplicationForm (1).pdf', 'R487E44ApplicationForm.pdf', 'Ramakrishnan - Database Management Systems 3rd Edition.pdf', 'README.md', 'Recording of lab experiment_Alkalinity_Dr Aditya Sharma (1).mp4', 'Reinforcement Learning BOOK (1).pdf', 'Reinforcement Learning BOOK (2).pdf', 'Reinforcement Learning BOOK (3).pdf', 'Reinforcement Learning BOOK.pdf', 'Report final year project.docx', 'REPORT on Developing an NLP-Based Question and Answering Cha - Copy (1).docx', 'REPORT on Developing an NLP-Based Question and Answering Cha - Copy.docx', 'REPORT on Developing an NLP-Based Question and Answering Cha.pdf', 'Report Synopsis _2024.docx', 'Report synopsis.docx', 'Resumae 11.png', 'Resume.pdf', 'RL', 'RL final project .ipynb', 'RL Grp.project (21csu278) 5th Sem.ipynb', 'RL lab manual 21csu288.docx', 'RL project', 'RL PROJECT (1).ipynb', 'RL_PROJECT_GROUP-4nn.docx', 'RoboAnalyzer V8.0.1', 'roboanalyzer_7_5_20190623 (1)', 'Siddharth Rana  21csu271 focp practical 1.docx', 'siddharth21csu271 prct.2 focp.docx', 'sldim', 'smart home Exp on 19th nov.pkt', 'smart home.pkt', 'Solidworks', 'SolidWorksSetup (1).exe', 'SolidWorksSetup.exe', 'Soumyadeep 21csu556 pfds lab manual 2022-23.docx', 'Sowmya_21csu326 (1).docx', 'Sowmya_21CSU326.docx', 'SOWMYA_21CSU326_CS Lab Workbook.docx', 'spark-3.4.1-bin-hadoop3.tgz', 'Star Topology.pkt', 'Statc Routing.pkt', 'Static Routing correct (1).pkt', 'Static Routing correct (2).pkt', 'Static Routing correct.pkt', 'Static Routing.pptx', 'Static.pkt', 'Statistical Estimation of Mean for a one population using z-test .pptx', 'Stranger.Things.S04E07.720p.WEB-DL.x265.TagName.mkv', 'Stranger.Things.S04E08.720p.WEB-DL.x265.TagName.mkv', 'Stranger.Things.S04E09.720p.WEB-DL.x265.TagName.mkv', 'Stremio+4.4.168.exe', 'Sublime Text Build 3211 x64 Setup.exe', 'Summer Internship Report (1) (1).docx', 'Summer Internship Report Format (1) - Copy (1).pdf', 'sunday to tusday.zip', 'Supervisor‚Äôs comments.pdf', 'SVM AAISE13 (1).ipynb', 'SVM AAISE13.ipynb', 'switch case.java', 'Syntax (1).txt', 'Syntax.txt', 'Tangled.(NKIRI.COM).2010.Downloaded.from.NKIRI.COM.mkv', 'TeamsSetup_c_w_ (1).exe', 'TeamsSetup_c_w_.exe', 'Teams_windows_x64.exe', 'TeraBox_sl_b_1.34.0.4.exe', 'Tesla, Inc. (TSLA) Stock Historical Prices & Data - Yahoo Finance_files', 'tesla-stock-price.csv', 'test (1) - Copy.csv', 'test (1).csv', 'test (2).csv', 'Titanic_precdiction.csv.ipynb', 'TLauncher-2.885-Installer-1.1.3.exe', 'train (1).csv', 'train (2).csv', 'transactions (1).tgz', 'transactions.tgz', 'TSLA.csv', 'twia-1-2 (1) (1) (1).pptx', 'twia-1-2 (1) (1).pptx', 'undefined (1).erdplus', 'undefined.erdplus', 'Unit-1 PM&B.pptx', 'UNIT-3__16_AUG_2023.pptx', 'UNIT-3__23_AUG_2023_Part_2.pptx', 'university_graph.html', 'Untitled Folder', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled12 (2).ipynb', 'Untitled12.ipynb', 'Untitled16.ipynb', 'Untitled17.ipynb', 'Untitled18.ipynb', 'Untitled19.ipynb', 'uptodown-com.studiowildcard.wardrumstudios.ark (1).apk', 'uptodown-com.studiowildcard.wardrumstudios.ark.apk', \"utf-8''Cartwheeldata.csv\", \"utf-8''nhanes_2015_2016.csv\", 'VA', 'VA Assignment (Personality Test) - Copy.docx', 'VA Assignment (Personality Test).docx', 'VC_redist.x64.exe', 'ventoy-1.0.91-windows.zip', 'vision11.apk', 'vlan.pkt', 'VMware-workstation-full-17.0.0-20800274.exe', 'Web server.pkt', 'webpage 123.html', 'webpage.html', 'Wednesday.S01.720p.WEB-DL.Hindi.English.AAC.5.1.ESub.x264-HDHub4u.Tv.zip', 'Week 1 Quiz.docx', 'Week 2 Quiz.docx', 'Week 3 Quiz.docx', 'Week 4 Quiz (1).docx', 'Week 4 Quiz.docx', 'Week 5 Quiz (1).docx', 'Week 5 Quiz.docx', 'WhatsApp Image 2023-02-26 at 21.22.05.jpg', 'WhatsApp Image 2023-03-12 at 09.09.56.jpg', 'WhatsApp Image 2023-03-12 at 09.10.14.jpg', 'WhatsApp Image 2023-03-12 at 09.10.42.jpg', 'WhatsApp Image 2023-07-03 at 12.30.28 PM.jpeg', 'WhatsApp Image 2023-08-27 at 12.46.05 PM.jpeg', 'WhatsApp Image 2023-09-23 at 6.13.55 PM.jpeg', 'WhatsApp Image 2023-09-27 at 10.07.46 PM.jpeg', 'WhatsApp Image 2023-09-27 at 10.07.51 PM.jpeg', 'WhatsApp Image 2023-09-27 at 10.09.39 PM.jpeg', 'WhatsApp Image 2023-09-27 at 10.09.40 PM.jpeg', 'WhatsApp Image 2023-09-27 at 10.09.48 PM.jpeg', 'WhatsApp Image 2023-09-27 at 10.09.49 PM.jpeg', 'WhatsApp Image 2023-11-27 at 12.31.38 PM.jpeg', 'WhatsApp Image 2024-09-26 at 13.15.47_37b06aa0.jpg', 'WhatsApp Image 2024-09-26 at 13.15.49_b5b488b0.jpg', 'WhatsApp Image 2024-09-26 at 13.21.29_5e8dbc72.jpg', 'WhatsApp Image 2024-09-26 at 13.21.29_5e8dbc72.pdf', 'WhatsApp Image 2024-12-05 at 07.48.02_d7a4f6bb.jpg', 'WhatsApp Image 2024-12-05 at 07.55.12_f34b6c31.jpg', 'WhatsApp Image 2024-12-24 at 15.23.04_dcc1a2d7.jpg', 'WhatsApp Image 2024-12-24 at 15.58.31_cc60953c.jpg', 'WhatsApp Image 2024-12-24 at 16.04.37_2439427b.jpg', 'WhatsAppSetup.exe', 'Wireshark-win64-4.0.1.exe', 'x2.png', 'x3.png', 'x4.png', 'x5.png', 'x6.png', 'x7.png', 'x8.png', 'Yatin Vadehra, 21ecu007, photograph.jpg', 'yelp-easy.jsonl', 'yelp-hard.jsonl', 'yoga.jpeg', 'Zoom_cm_fo42pnktZ9vvrZo4_m70lfjPzZGpHTnWpfERKl1wQewiw7lmMYo9ZY@oCEby8aIHjS1qWhU_kd8581944327778bc_.exe', '~$ assignment security tools.docx', '~$ lab manual 21csu288.docx', '~$ Lab manual Aditya Sindhu(21csu278).docx', '~$ Workbook CSL303.docx', '~$HVPE Project.pptx', '~$IL_EX19_CS1-4a_FirstLastName_1.xlsx', '~$itya SINDHU 21CSU278 DBMS LAN MANUAL.docx', '~$ML-B Snigdha21csu283.docx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List files in current directory\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files present:\", os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c3530b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who teaches CSS101?\n",
      "A: Normal query.\n",
      "\n",
      "Q: What is the syllabus for aptitude exam?\n",
      "A: .\n",
      "\n",
      "Q: Who is the faculty for Data Visualization (CSV201)?\n",
      "A: .\n",
      "\n",
      "Q: When is the exam?\n",
      "A: Exam is on Monday.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with error cases\n",
    "test_cases = [\n",
    "    (\"Who teaches CSS101?\", \"Normal query\"),\n",
    "    (\"What is the syllabus for aptitude exam?\", \"\"),  # Empty context\n",
    "    (\"Who is the faculty for Data Visualization (CSV201)?\", \"   \"),  # Whitespace context\n",
    "    (\"When is the exam?\", \"Exam is on Monday\")  # Short answer\n",
    "]\n",
    "\n",
    "for question, context in test_cases:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer_generator.generate_answer(question, context)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "abaa95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Robust Answer Generator\n",
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        try:\n",
    "            # Handle empty context\n",
    "            if not context or not context.strip():\n",
    "                return \"I don't have information about this topic.\"\n",
    "            \n",
    "            # Improved sentence splitting\n",
    "            sentences = [s.strip() for s in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context) if s.strip()]\n",
    "            \n",
    "            if not sentences:\n",
    "                return context[:500] + ('...' if len(context) > 500 else '')\n",
    "            \n",
    "            # Get most relevant sentences\n",
    "            question_vec = self.vectorizer.fit_transform([question])\n",
    "            sentence_vecs = self.vectorizer.transform(sentences)\n",
    "            similarities = cosine_similarity(question_vec, sentence_vecs).flatten()\n",
    "            \n",
    "            # Get top 3 relevant sentences\n",
    "            top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "            best_answers = []\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                if similarities[idx] > 0.1:  # Minimum similarity threshold\n",
    "                    best_answers.append(sentences[idx])\n",
    "            \n",
    "            if not best_answers:\n",
    "                return context[:500] + ('...' if len(context) > 500 else '')\n",
    "            \n",
    "            # Combine best answers\n",
    "            final_answer = ' '.join(best_answers)\n",
    "            \n",
    "            # Clean up formatting\n",
    "            final_answer = final_answer.replace(\"  \", \" \").strip()\n",
    "            if not final_answer:\n",
    "                return context[:500] + ('...' if len(context) > 500 else '')\n",
    "            \n",
    "            # Ensure proper punctuation\n",
    "            if not final_answer.endswith(('.','!','?')):\n",
    "                final_answer += '.'\n",
    "            \n",
    "            return final_answer[0].upper() + final_answer[1:]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug Error: {str(e)}\")\n",
    "            return context[:500] + ('...' if len(context) > 500 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5ab0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Chatbot Class\n",
    "class CollegeChatbot:\n",
    "    def __init__(self, pdf_path, qa_path):\n",
    "        self.processor = TextProcessor()\n",
    "        self.answer_gen = AnswerGenerator()\n",
    "        \n",
    "        # Load data\n",
    "        self.pdf_text = self.processor.extract_text_from_pdf(pdf_path)\n",
    "        self.qa_pairs = self.processor.extract_qa_from_txt(qa_path)\n",
    "        \n",
    "        # Prepare embeddings\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.pdf_chunks = self.processor.chunk_text(self.pdf_text)\n",
    "        self.qa_questions = [qa['question'] for qa in self.qa_pairs]\n",
    "        \n",
    "        # Create combined embeddings\n",
    "        all_texts = self.pdf_chunks + self.qa_questions\n",
    "        self.embeddings = self.model.encode(all_texts)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = faiss.IndexFlatL2(self.embeddings.shape[1])\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "    def query(self, question):\n",
    "        # 1. Check Q&A pairs first\n",
    "        question_embed = self.model.encode([question])\n",
    "        qa_similarities = np.dot(\n",
    "            self.embeddings[len(self.pdf_chunks):], \n",
    "            question_embed.T\n",
    "        ).flatten()\n",
    "        best_qa_idx = np.argmax(qa_similarities)\n",
    "        \n",
    "        if qa_similarities[best_qa_idx] > 0.7:  # Similarity threshold\n",
    "            return self.qa_pairs[best_qa_idx]['answer']\n",
    "        \n",
    "        # 2. Search PDF chunks\n",
    "        _, pdf_indices = self.index.search(question_embed.astype('float32'), k=3)\n",
    "        context = ' '.join([\n",
    "            self.pdf_chunks[idx] \n",
    "            for idx in pdf_indices[0] \n",
    "            if idx < len(self.pdf_chunks)\n",
    "        ])\n",
    "        \n",
    "        if not context.strip():\n",
    "            return \"I couldn't find relevant information in the documents.\"\n",
    "        \n",
    "        return self.answer_gen.generate_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3b5419ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who teaches CSS101?\n",
      "A: The Community Service (CSS101) course is taught by Ms. Garima Sharma, Dr. Sandeep Singh, Dr. Rita Chhikara, Dr. Anvesha Katti, and Dr. Yogita Gigra.\n",
      "\n",
      "Q: What is the syllabus for aptitude exam?\n",
      "A: I couldn't find relevant information in the documents.\n",
      "\n",
      "Q: Who is the faculty for Data Visualization (CSV201)?\n",
      "A: Prof. Michael Tibbetts is the faculty for Data Visualization (CSV201).\n",
      "\n",
      "Q: When is the exam?\n",
      "A: As this is the project for first year students therefore, we are also having the data for the Aptitute exam preparation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Test Cases\n",
    "bot = CollegeChatbot(PDF_PATH, QA_PATH)\n",
    "\n",
    "test_questions = [\n",
    "    \"Who teaches CSS101?\",\n",
    "    \"What is the syllabus for aptitude exam?\",\n",
    "    \"Who is the faculty for Data Visualization (CSV201)?\",\n",
    "    \"When is the exam?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {bot.query(question)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "06c6e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr  # For the web interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba697fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_interface(question, history):\n",
    "    return bot.query(question)\n",
    "\n",
    "gr.ChatInterface(\n",
    "    chat_interface,\n",
    "    title=\"NCU College Assistant\",\n",
    "    description=\"Ask about courses, faculty, or exam preparation\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155d64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5d0c91",
   "metadata": {},
   "source": [
    "## Extra Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62c869a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA VERIFICATION ===\n",
      "Loaded 37 PDF chunks\n",
      "Sample PDF chunk: This Word is the dataset for the Final Year project. In this dataset we are taking the First year co...\n",
      "\n",
      "Loaded 41 Q&A pairs\n",
      "Sample Q&A: {'question': 'Who teaches the Community Service (CSS101) course?', 'answer': 'The Community Service (CSS101) course is taught by Ms. Garima Sharma, Dr. Sandeep Singh, Dr. Rita Chhikara, Dr. Anvesha Katti, and Dr. Yogita Gigra.'}\n"
     ]
    }
   ],
   "source": [
    "# Add this right after initializing your bot\n",
    "print(\"\\n=== DATA VERIFICATION ===\")\n",
    "print(f\"Loaded {len(bot.pdf_chunks)} PDF chunks\")\n",
    "print(\"Sample PDF chunk:\", bot.pdf_chunks[0][:100] + \"...\")\n",
    "print(f\"\\nLoaded {len(bot.qa_pairs)} Q&A pairs\")\n",
    "print(\"Sample Q&A:\", bot.qa_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7237d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF document\"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    \n",
    "    def extract_qa_from_txt(self, txt_path):\n",
    "        \"\"\"Improved Q&A extraction that handles various formats\"\"\"\n",
    "        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Handle different Q&A formats\n",
    "        qa_pairs = []\n",
    "        current_q = None\n",
    "        current_a = []\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith(('Q.', 'Q:', 'Q ')) or line.startswith('Q'):\n",
    "                if current_q is not None:\n",
    "                    qa_pairs.append({\n",
    "                        'question': current_q,\n",
    "                        'answer': ' '.join(current_a)\n",
    "                    })\n",
    "                current_q = line[2:].strip() if line.startswith(('Q.', 'Q:')) else line[1:].strip()\n",
    "                current_a = []\n",
    "            elif line.startswith(('A.', 'A:', 'A ')) or line.startswith('A'):\n",
    "                answer_part = line[2:].strip() if line.startswith(('A.', 'A:')) else line[1:].strip()\n",
    "                current_a.append(answer_part)\n",
    "            elif current_a:  # Continuation of answer\n",
    "                current_a.append(line)\n",
    "        \n",
    "        if current_q is not None:\n",
    "            qa_pairs.append({\n",
    "                'question': current_q,\n",
    "                'answer': ' '.join(current_a)\n",
    "            })\n",
    "            \n",
    "        return qa_pairs\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=200):\n",
    "        \"\"\"Split text into chunks of approximately chunk_size words\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            current_chunk.append(word)\n",
    "            current_count += 1\n",
    "            \n",
    "            if current_count >= chunk_size:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_count = 0\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,;?]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5373df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CollegeChatbot:\n",
    "    def __init__(self, pdf_path, qa_path):\n",
    "        self.processor = TextProcessor()\n",
    "        self.qa_pairs = self.processor.extract_qa_from_txt(qa_path)\n",
    "        \n",
    "        # Create question-answer mapping for exact matches\n",
    "        self.qa_map = {pair['question'].lower().strip(): pair['answer'] \n",
    "                      for pair in self.qa_pairs}\n",
    "        \n",
    "        # Initialize PDF processing if needed\n",
    "        self.pdf_text = \"\"\n",
    "        if pdf_path and os.path.exists(pdf_path):\n",
    "            self.pdf_text = self.processor.extract_text_from_pdf(pdf_path)\n",
    "            self.pdf_chunks = self.processor.chunk_text(self.pdf_text)\n",
    "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            self.embeddings = self.model.encode(self.pdf_chunks)\n",
    "            self.index = faiss.IndexFlatL2(self.embeddings.shape[1])\n",
    "            self.index.add(self.embeddings.astype('float32'))\n",
    "    \n",
    "    def query(self, question):\n",
    "        # 1. First check for exact match (case insensitive)\n",
    "        normalized_question = question.lower().strip()\n",
    "        if normalized_question in self.qa_map:\n",
    "            return self.qa_map[normalized_question]\n",
    "        \n",
    "        # 2. Check for partial matches in questions\n",
    "        for q, a in self.qa_map.items():\n",
    "            if q in normalized_question or normalized_question in q:\n",
    "                return a\n",
    "        \n",
    "        # 3. If no Q&A match, search PDF (if available)\n",
    "        if hasattr(self, 'index'):\n",
    "            question_embed = self.model.encode([question])\n",
    "            _, pdf_indices = self.index.search(question_embed.astype('float32'), k=1)\n",
    "            if pdf_indices[0][0] < len(self.pdf_chunks):\n",
    "                return self.pdf_chunks[pdf_indices[0][0]]\n",
    "        \n",
    "        return \"I couldn't find information about this topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38bb3108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who teaches the Community Service (CSS101) course?\n",
      "A: The Community Service (CSS101) course is taught by Ms. Garima Sharma, Dr. Sandeep Singh, Dr. Rita Chhikara, Dr. Anvesha Katti, and Dr. Yogita Gigra. \n",
      "\n",
      "Q: What is the email address of Dr. Sandeep Singh?\n",
      "A: Dr. Sandeep Singh‚Äôs email address is sandeepsingh@ncuindia.edu. \n",
      "\n",
      "Q: Which room is Dr. Aditya Sharma available in for Engineering Chemistry?\n",
      "A: This Word is the dataset for the Final Year project. In this dataset we are taking the First year courses and their faculty members details. The labs room location in the campus. Add on to it we are taking info of Clubs, Events and of library. As this is the project for first year students therefore, we are also having the data for the Aptitute exam preparation. First comes, the first-year course and the faculty members for that: Course Course Code Facalty ID Room No. Community Service CSS101 Ms. Garima Sharma garimasharma@ncuindi a.edu 125 Dr.Sandeep Singh sandeepsingh@ncuindia .edu 108 Dr.Rita Chhikara ritachhikara@ncuindia.e du 134 Dr.Anvesha Katti anveshakatti@ncuindia. edu 130 Dr.Yogita Gigra yogitagigras@ncuindia.e du 125 Engineering Chemistry CHL150 Dr.Aditya Sharam adityasharma@ncuindia .edu 29 Dr. Bharti Arora bhartiarora@ncuindia.e du 11 Dr. Tejpal Singh chundawat tejpal@ncuindia.edu 29 Engineering Graphic and Drawing MEP110 Dr. Akanksha Mathur akanshamathur@ncuind ia.edu 224 Engineering Mathematics-1 MAL151 Dr. Rajini Rohila rajnirohila@ncuindia.ed u 29 Dr. Tarul Garg tarulgarg@ncuindia.edu 32 Dr. Saraswati Shah sarswatishah@ncuindia. edu 32 Miss Juhi Chaudhary juhichaudhary@ncuindia .edu 32 FOCP 1 CSL106 Dr. Neeti Kashyap neetikashyap@ncuindia. edu Ms. Bharti bharti19csd002@ncuind ia.edu Ms. Preeti preetithareja@ncuindia. edu Dr. Mrinal mrinalpandey@ncuindia .edu Ms. Monika Lamba monikalamba@ncuindia .edu Mr. Anand anand@ncuindia.edu Ms.\n",
      "\n",
      "Q: What is the syllabus for the Aptitude Exam at NCU?\n",
      "A: The syllabus includes the following topics: Problems on Trains Time and Distance Height and Distance Time and Work Simple Interest Compound Interest Profit and Loss Partnership Percentage Problems on Ages Calendar Clock verage rea Volume and Surface Area Permutation and Combination Numbers Problems on Numbers Problems on H.C.F and L.C.M Decimal Fraction Simplification Square Root and Cube Root Surds and Indices Ratio and Proportion Chain Rule Pipes and Cistern Boats and Streams lligation or Mixture Logarithm Races and Games Stocks and Shares Probability True Discount Banker's Discount Odd Man Out and Series \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize with your paths\n",
    "bot = CollegeChatbot(\n",
    "    pdf_path=\"Aptitude Exam Preparation Guide.pdf\",\n",
    "    qa_path=\"questionnanswers.txt\"\n",
    ")\n",
    "\n",
    "# Test with your questions\n",
    "test_questions = [\n",
    "    \"Who teaches the Community Service (CSS101) course?\",\n",
    "    \"What is the email address of Dr. Sandeep Singh?\",\n",
    "    \"Which room is Dr. Aditya Sharma available in for Engineering Chemistry?\",\n",
    "    \"What is the syllabus for the Aptitude Exam at NCU?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {bot.query(q)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c4523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1348ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(self, question):\n",
    "    print(f\"\\nProcessing question: '{question}'\")\n",
    "    \n",
    "    # 1. Check Q&A pairs first\n",
    "    question_embed = self.model.encode([question])\n",
    "    qa_similarities = np.dot(\n",
    "        self.embeddings[len(self.pdf_chunks):], \n",
    "        question_embed.T\n",
    "    ).flatten()\n",
    "    best_qa_idx = np.argmax(qa_similarities)\n",
    "    \n",
    "    print(f\"Best Q&A match: {self.qa_pairs[best_qa_idx]['question']} (score: {qa_similarities[best_qa_idx]:.2f})\")\n",
    "    \n",
    "    if qa_similarities[best_qa_idx] > 0.7:  # Only use if highly confident\n",
    "        return self.qa_pairs[best_qa_idx]['answer']\n",
    "    \n",
    "    # 2. Search PDF chunks\n",
    "    _, pdf_indices = self.index.search(question_embed.astype('float32'), k=3)\n",
    "    context = ' '.join([\n",
    "        self.pdf_chunks[idx] \n",
    "        for idx in pdf_indices[0] \n",
    "        if idx < len(self.pdf_chunks)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Retrieved PDF context: {context[:200]}...\")\n",
    "    \n",
    "    if not context.strip():\n",
    "        return \"I couldn't find relevant information in our documents.\"\n",
    "    \n",
    "    return self.answer_gen.generate_answer(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98dbbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGenerator:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def generate_answer(self, question, context):\n",
    "        try:\n",
    "            # Handle cases where the context already contains the exact answer\n",
    "            if \"?\" in question and question.split(\"?\")[0].lower() + \"?\" in context.lower():\n",
    "                relevant_part = context.split(question.split(\"?\")[0].lower() + \"?\")[1].split(\".\")[0]\n",
    "                return relevant_part.strip().capitalize()\n",
    "            \n",
    "            # Fallback to our previous robust method\n",
    "            sentences = [s.strip() for s in re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context) if s.strip()]\n",
    "            \n",
    "            if not sentences:\n",
    "                return context[:300] + ('...' if len(context) > 300 else '')\n",
    "            \n",
    "            # Find the sentence that contains the most question keywords\n",
    "            question_keywords = set(re.findall(r'\\w+', question.lower()))\n",
    "            best_sentence = max(\n",
    "                sentences,\n",
    "                key=lambda s: len(question_keywords.intersection(set(re.findall(r'\\w+', s.lower())))),\n",
    "                default=\"\"\n",
    "            )\n",
    "            \n",
    "            return best_sentence if best_sentence else context[:300] + '...'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Answer generation error: {e}\")\n",
    "            return context[:300] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d1faba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING ===\n",
      "\n",
      "Q: Who teaches CSS101?\n",
      "A: The Community Service (CSS101) course is taught by Ms. Garima Sharma, Dr. Sandeep Singh, Dr. Rita Chhikara, Dr. Anvesha Katti, and Dr. Yogita Gigra.\n",
      "---\n",
      "\n",
      "Q: What is Dr. Singh's email?\n",
      "A: Dr. Sandeep Singh‚Äôs email address is sandeepsingh@ncuindia.edu.\n",
      "---\n",
      "\n",
      "Q: Where is the library located?\n",
      "A: I couldn't find relevant information in the documents.\n",
      "---\n",
      "\n",
      "Q: When are the semester exams?\n",
      "A: This Word is the dataset for the Final Year project. In this dataset we are taking the First year courses and their faculty members details. The labs room location in the campus. Add on to it we are taking info of Clubs, Events and of library. As this is the project for first year students therefore, we are also having the data for the Aptitute exam preparation. First comes, the first-year course and the faculty members for that: Course Course Code Facalty ID Room No. Community Service CSS101 Ms...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"Who teaches CSS101?\",\n",
    "    \"What is Dr. Singh's email?\",\n",
    "    \"Where is the library located?\",\n",
    "    \"When are the semester exams?\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTING ===\")\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {bot.query(q)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e033a3e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2748382321.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [130]\u001b[1;36m\u001b[0m\n\u001b[1;33m    | Stage               | Methods                          | Online Tools/Models               | Offline Alternatives               |\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "| Stage               | Methods                          | Online Tools/Models               | Offline Alternatives               |\n",
    "|---------------------|----------------------------------|------------------------------------|------------------------------------|\n",
    "| **Data Collection** | PDF/Q&A text extraction          | `PyPDFLoader`, `TextLoader`        | `pdfplumber`, `PyMuPDF`            |\n",
    "| **Preprocessing**   | Text chunking, context preservation | `RecursiveCharacterTextSplitter`   | `NLTK`, `spaCy` sentence splitting |\n",
    "| **Vector Database** | Embedding + semantic search      | `all-MiniLM-L6-v2` + `FAISS`       | `FastText` + `Annoy`               |\n",
    "| **LLM Integration** | Context-aware generation         | **Online**:<br>- `GPT-3.5`<br>- `Llama-2-7b`<br>**T5**:<br>- `flan-t5-large` | **Offline**:<br>- `flan-t5-base` (quantized)<br>- `Zephyr-7b` (4-bit) |\n",
    "| **QA System**       | Hybrid retrieval + prompting     | `RetrievalQA` (LangChain)          | TF-IDF + rule-based matching       |\n",
    "| **Aptitude Module** | Question storage + test generation | `SQLite`                           | `JSON`/`CSV` files                 |\n",
    "| **Chat Interface**  | Real-time interaction            | `Gradio`/`Streamlit`               | `Tkinter` (local GUI)              |\n",
    "| **Deployment**      | Containerization + hosting       | `Docker` + `Hugging Face Spaces`   | Local server (`FastAPI`)            |\n",
    "| **Monitoring**      | Interaction logging              | `CSV`/`SQLite` + `Weights & Biases`| Manual logging                     |\n",
    "\n",
    "### **Model Specs**\n",
    "| Model               | Type       | Hardware  | Use Case                     |\n",
    "|---------------------|------------|-----------|------------------------------|\n",
    "| `flan-t5-small`     | T5 (offline) | CPU       | Basic Q&A                    |\n",
    "| `flan-t5-base`      | T5 (offline) | CPU/GPU   | Balanced speed/accuracy       |\n",
    "| `Zephyr-7b`         | LLM (offline)| GPU       | High-accuracy local deployment|\n",
    "| `Llama-2-7b`        | LLM (offline)| GPU       | Privacy-focused apps          |\n",
    "| `GPT-3.5`           | LLM (online) | API       | Production deployments        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee4f5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0e888f",
   "metadata": {},
   "source": [
    "| Stage               | Methods                          | Online Tools/Models               | Offline Alternatives               |\n",
    "|---------------------|----------------------------------|------------------------------------|------------------------------------|\n",
    "| **Data Collection** | PDF/Q&A text extraction          | `PyPDFLoader`, `TextLoader`        | `pdfplumber`, `PyMuPDF`            |\n",
    "| **Preprocessing**   | Text chunking                    | `RecursiveCharacterTextSplitter`   | `NLTK`, `spaCy`                    |\n",
    "| **Vector Database** | Embedding + semantic search      | `all-MiniLM-L6-v2` + `FAISS`       | `FastText` + `Annoy`               |\n",
    "| **LLM Integration** | Context-aware generation         | `GPT-3.5`, `Llama-2-7b`            | `flan-t5-base`, `Zephyr-7b`        |\n",
    "| **QA System**       | Hybrid retrieval                 | `RetrievalQA` (LangChain)          | TF-IDF + rule-based                |\n",
    "| **Aptitude Module** | Question storage                 | `SQLite`                           | `JSON`/`CSV` files                 |\n",
    "| **Chat Interface**  | Real-time interaction            | `Gradio`/`Streamlit`               | `Tkinter`                          |\n",
    "| **Deployment**      | Containerization                 | `Docker` + `Hugging Face Spaces`   | `FastAPI` (local)                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc37bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
